
<html>
<center>
<head>
<title>
Soumyabrata Pal
</title>
<link rel="stylesheet" type="text/css" href="styles.css">
</head>
<body>


<div style="border-right: 0px solid
  black;text-align:left;padding:30px;max-width:850px">
<table>
<tr>
<td width=800>
  <h2>Soumyabrata Pal</h2>
<p>
<a href="https://research.adobe.com/careers/bangalore/"> Adobe Research, India</a><br/>
email: soumyabratapal13 at gmail dot com, soumyabratap at adobe dot com<br/>
<A 
HREF="https://scholar.google.com/citations?user=J4UxoTEAAAAJ&hl=en">[Google scholar]</A>
<A 
HREF="https://dblp.org/pid/206/6371.html">[DBLP]</A>
<A 
HREF="https://www.overleaf.com/read/tknkndctyqrz#4c68fe">[Research Statement]</A>
  

</td>
<td>
 <img src="soumya2.jpg" width="240" height="180" style="transform:rotate(0deg);">
</td>
</tr>
</table>
<h3> About me </h3>


 <p>
 Currently, I am a Research Scientist at Adobe Research, Bangalore (India). Prior to this, I spent two years as a postdoc at Google Research, India working with <a href="https://www.prateekjain.org/"> Dr. Prateek Jain </a> and <a href="https://sites.google.com/a/utexas.edu/karthiksh/"> Dr. Karthikeyan Shanmugam </a>. Before that, I completed my Ph.D in the Computer Science Department (CICS) at the University of Massachusetts Amherst advised by <a href="https://mazumdar.ucsd.edu/"> Dr. Arya Mazumdar</a>. 
During that time, I was a Visiting Graduate Student at the University of California San Diego from May - November 2021. I had also spent the summer of 2019 as a Research Intern at Ernst & Young AI Lab at Palo Alto and Spring 2020 as an Applied Scientist Intern at Amazon Search (Berkeley).   
Even earlier, I graduated from <a href="http://www.iitkgp.ac.in/">Indian Institute of Technology, Kharagpur</a> in August 2016 with a Bachelor's
degree in Electronics and Electrical Communication Engineering. 
</p>

  
<h3> Research</h3>

  <p>
    My research interests are LLM Efficieny and Theoretical Machine Learning focused on Non-convex Optimization and Online Learning. More concisely, I love Statistical recovery/reconstruction problems under different reasonable structural assumptions on the data generating mechanism such as sparsity, low-rank, presence of latent clusters among others. 
    Nowadays, I am working on designing algorithms in offline/online/hybrid systems aimed at incorporating personalization efficiently at scale. 
    Most of my work so far can be categorized into five topics namely 1) Scalable Personalization via Low Rank and Sparse Decomposition 2) Multi-agent Online Learning via Collaborative Filtering 3) Latent Variable models - Mixtures of Linear Regression, Linear Classifiers and Distributions 4) Generative models for Graph Clustering - Geometric Block Model
    and 5) Active learning for Semi-supervised clustering - Disjoint Clusters, Overlapping Clusters and Fuzzy Clusters
 </p>

<h3> Recent News</h3>

  <p>
    2 New papers in NeurIPS 2023. 
  </p>
  <p>
    New papers in JMLR, ICLR 2023, AISTATS 2023. 
  </p>
  <p>
    New paper in ALT 2022!! Also started as a postdoc at Google Research!
  </p>
  <p>
    2 new papers in NeurIPS 2021 and 1 new paper in ITCS !!
  </p>
  <p>
    Started as a Visiting Graduate Student at UCSD from May 2021!!
  </p>  
  
  
 

<div></div>
<span></span>
  

<h3> PhD Dissertation </h3>   <p> </p>

<p> </p>
<!-- <span style="font-size:16px;font-style:italic"> Statistical learning theory</span>  -->
<p>

<ol>

<li><p><a href="https://scholarworks.umass.edu/dissertations_2/2437/"> Mixture Models in Machine Learning </a> 
</li>  
  
    
</ol>


</p>
  
  
  
<h3> Preprints </h3>   <p> </p>

<p> </p>
<!-- <span style="font-size:16px;font-style:italic"> Statistical learning theory</span>  -->
<p>

<ol>
  
 <li><p> Improved Algorithms for Stochastic Linear Bandits with Prior Observations via D-optimal Exploration </a> <br />
with Sushant Vijayan, Karthikeyan Shanmugam and Arun Sai Suggala <br /> 
In Submission.
</li>    

   

<li><p> Online Matrix Completion: A Collaborative Approach with Hott Items </a> <br />
with Dheeraj Baby <br /> 
In Submission.
</li> 
  


<li><p><a href="http://arxiv.org/abs/2110.00744"> Random Subgraph Detection Using Queries </a> <br />
with Wasim Huleihel and Arya Mazumdar<br /> 
Submitted to  Journal of Machine Learning Research (JMLR).<br /></p>

  
<li><p><a href="https://arxiv.org/abs/2202.11940">Support Recovery in Mixture Models with Sparse Parameters</a><br />
with Arya Mazumdar<br />
Submitted to Journal of Machine Learning Research (JMLR) - under Revision. Shorter version titled "On Learning Mixture Models with Sparse Parameters " accepted at AISTATS 2022<br /></p>
</li> 
  
    
</ol>


</p>
  
  
  

<h3> Journal Publications </h3>   <p> </p>

<p> </p>
<!-- <span style="font-size:16px;font-style:italic"> Statistical learning theory</span>  -->
<p>

<ol>

<li><p><a href="https://arxiv.org/abs/2210.16657"> Improved Support Recovery in One-Bit Compressed Sensing </a> <br />
with Namiko Matsumoto and Arya Mazumdar<br />
IEEE Transactions on Information Theory, 2024. Preliminary version appeared in Innovations in Theoretical Computer Science (ITCS), 2022.<br /></p>
</li>    
  
<li><p><a href="https://arxiv.org/abs/2206.11303">Community Recovery in the Geometric Block Model</a><br />
with Sainyam Galhotra, Arya Mazumdar and Barna Saha<br />
To appear, Journal of Machine Learning Research (JMLR), 2023. Shorter versions accepted in RANDOM 2019 and AAAI 2018<br /></p>
</li>      

<li><p><a href="https://arxiv.org/abs/1904.09618">Trace Reconstruction: Generalized and Parameterized</a> <br />
with Akshay Krishnamurthy, Arya Mazumdar and Andrew McGregor<br />
IEEE Transactions on Information Theory, 2021. Preliminary version appeared in European Symposium on Algorithms (ESA), 2019.</p>
</li>  
  
  
<li><p><a href="https://arxiv.org/abs/1904.00507"> Semisupervised Clustering by Queries and Locally Encodable Source Coding </a> <br />
with Arya Mazumdar<br />
IEEE Transactions on Information Theory, 2021. Preliminary version appeared in Advances in Neural Information Processing Systems (NeurIPS), 2017.
</li>

</ol>


</p>


<h3> Conference Publications </h3>   <p> </p>
<span style="font-size:13pt;font-style:bold"> By year </span> &nbsp;&nbsp;&nbsp;

<p> </p>
<!-- <span style="font-size:16px;font-style:italic"> Statistical learning theory</span>  -->
<p>

<ol>

<li><p><a href="https://arxiv.org/pdf/2210.03505.pdf"> Private and Efficient Meta-Learning with Low Rank and Sparse
Decomposition </a> <br />
with Prateek Varshney, Prateek Jain, Abhradeep Guha Thakurta, Gagan
Madan, Gaurav Aggarwal, Pradeep Shenoy, and Gaurav Srivastava <br /> 
International Conference on Artificial Intelligence and Statistics (AISTATS), 2024.
</li>     

<li><p><a href="https://teamcore.seas.harvard.edu/sites/projects.iq.harvard.edu/files/teamcore/files/kilikari_aamas_camera_ready_1.pdf"> Improving Mobile Maternal and Child Health Care Programs:
Collaborative Bandits for Time slot selection </a> <br />
with Milind Tambe, Arun Suggala, Karthikeyan Shanmugam, and Aparna Taneja <br /> 
International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2024.
</li> 
  
<li><p><a href="https://arxiv.org/abs/2311.03376"> Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints </a> <br />
with Arun Sai Suggala, Karthikeyan Shanmugam and Prateek Jain<br /> 
Advances in Neural Information Processing Systems (NeurIPS), 2023.</p>
</li>  

<li><p><a href="https://arxiv.org/abs/2310.02023"> Nash Regret Guarantees for Linear Bandits </a> <br />
with Siddharth Barman and Ayush Sawarni <br /> 
Advances in Neural Information Processing Systems (NeurIPS), 2023.</p>
</li>     
  
<li><p><a href="https://arxiv.org/abs/2301.07040"> Optimal Algorithms for Latent Bandits with Cluster Structure </a> <br />
with Arun Sai Suggala, Karthikeyan Shanmugam and Prateek Jain<br /> 
International Conference on Artificial Intelligence and Statistics (AISTATS), 2023.
</li>    
  
<li><p><a href="https://arxiv.org/abs/2209.03997"> Online Low Rank Matrix Completion </a> <br />
with Prateek Jain<br /> 
International Conference on Learning Representations (ICLR), 2023.
</li>  
  
<li><p><a href="https://arxiv.org/pdf/2205.13166.pdf"> On Learning Mixture of Linear Regressions in the Non-Realizable Setting </a> <br />
with Abhishek Ghosh, Arya Mazumdar and Rajat Sen<br />
International Conference on Machine Learning (ICML), 2022.
</li>    
 
<li><p><a href="https://arxiv.org/abs/2202.11940"> On Learning Mixture Models with Sparse Parameters  </a> <br />
with Arya Mazumdar<br />
International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.
</li>  
  
  
<li><p><a href="http://arxiv.org/abs/2109.01064"> Lower Bounds on the Total Variation Distance Between Mixtures of Two
 Gaussians </a> <br />
with Sami Davies, Arya Mazumdar and Cyrus Rashtchian<br />
  Algorithmic Learning Theory (ALT), 2022.</p> 
</li>  
  
<li><p><a href="https://arxiv.org/pdf/2107.09091"> Support Recovery in Universal One-bit Compressed Sensing </a> <br />
with Arya Mazumdar<br />
The 13th Innovations in Theoretical Computer Science (ITCS), 2022
</li>
  

<li><p><a href="https://arxiv.org/abs/2106.05951"> Support Recovery of Sparse Signals from a Mixture of Linear Measurements </a> <br />
with Venkata Gandikota and Arya Mazumdar<br />
Advances in Neural Information Processing Systems (NeurIPS), 2021.</p> 
</li>
  
 
<li><p><a href="https://arxiv.org/abs/2106.02212"> Fuzzy Clustering with Similarity Queries </a> <br />
with Wasim Huleihel and Arya Mazumdar<br />
Advances in Neural Information Processing Systems (NeurIPS), 2021.</p>  
</li>  
  
  
<li><p><a href="https://arxiv.org/abs/2101.12506"> Learning User Preferences in Non-Stationary Environments. </a> <br />
with Wasim Huleihel and Ofer Shayevitz<br />
International Conference on Artificial Intelligence and Statistics (AISTATS), 2021.</p>
</li>   
  
<li><p><a href="https://arxiv.org/abs/2010.12087"> Recovery of sparse linear classifiers from mixture of responses. </a> <br />
with Venkata Gandikota and Arya Mazumdar<br />
Advances in Neural Information Processing Systems (NeurIPS), 2020.</p>
</li>    
  
<li><p><a href="http://arxiv.org/abs/2006.16406"> Recovery of Sparse Signals from a Mixture of Linear Samples </a> <br />
with Arya Mazumdar<br />
International Conference on Machine Learning (ICML), 2020.</p>
</li>  
  
  
<li><p><a href="https://arxiv.org/abs/1806.11542">High Dimensional Discrete Integration by Hashing and Optimization</a> <br />
with Raj Kumar Maity and Arya Mazumdar<br />
Uncertainty in Artificial Intelligence (UAI), 2020.</p>
</li>  

<li><p><a href="https://arxiv.org/abs/2001.06776"> Algebraic and Analytic Approaches for Parameter Learning in Mixture Models  </a> <br />
with Akshay Krishnamurthy, Arya Mazumdar and Andrew McGregor<br />
 Algorithmic Learning Theory (ALT), 2020.</p> 
</li>
    
<li><p><a href="https://arxiv.org/abs/1910.12490">Same-Cluster Querying for Overlapping Clusters </a> <br />
with Wasim Huleihel, Arya Mazumdar and Muriel Medard<br />
 Advances in Neural Information Processing Systems (NeurIPS), 2019.</p>
</li>
  
<li><p><a href="https://arxiv.org/abs/1910.14106">Sample Complexity of Learning Mixture of Sparse Linear Regressions  </a> <br />
with Akshay Krishnamurthy, Arya Mazumdar and Andrew McGregor<br />
 Advances in Neural Information Processing Systems (NeurIPS), 2019.</p>
</li>
  
<li><p><a href="https://arxiv.org/abs/1904.09618">Trace Reconstruction: Generalized and Parameterized</a> <br />
with Akshay Krishnamurthy, Arya Mazumdar and Andrew McGregor<br />
European Symposium on Algorithms (ESA), 2019.</p>
</li>

  
<li><p><a href="https://arxiv.org/abs/1804.05013">Connectivity in Random Annulus Graphs and the Geometric Block Model</a><br />
with Sainyam Galhotra, Arya Mazumdar and Barna Saha<br />
International Conference on Randomization and Computation (RANDOM), 2019.<br /></p>
</li>  
    
<li><p><a href="https://arxiv.org/abs/1709.05510">The Geometric Block Model</a><br />
with Sainyam Galhotra, Arya Mazumdar and Barna Saha<br />
The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI), 2018.<br />
</li>

<li><p><a href="https://arxiv.org/abs/1904.00507">Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding</a> <br />
with Arya Mazumdar<br />
Advances in Neural Information Processing Systems (NeurIPS), 2017. <b><font color="red"> Spotlight</font></b></p>
</li>

</ol>


</p>

<h3> Workshop Publications </h3>   <p> </p>
<span style="font-size:13pt;font-style:bold"> By year </span> &nbsp;&nbsp;&nbsp;

<p> </p>
<!-- <span style="font-size:16px;font-style:italic"> Statistical learning theory</span>  -->
<p>

<ol>


<li><p><a href="https://arxiv.org/abs/1709.05510">The Geometric Block Model</a> <br />
with Sainyam Galhotra, Arya Mazumdar and Barna Saha <br />
NeurIPS 2017 Workshop on Learning on Distributions, Functions, Graphs and Groups, 2017.</p>
</li>

</ol>


</p>

</body>
</center>
</html>

